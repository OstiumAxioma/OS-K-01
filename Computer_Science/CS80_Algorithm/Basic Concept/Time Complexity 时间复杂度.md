---
Date: 2024-01-20
tags:
  - 计算机/编程语言/C
  - 计算机/数据结构
  - 计算机
  - 计算机/编程语言
Type:
  - Lecture
Year:
  - 2024 Spring
---
# 时间复杂度

运行时间可以直观且准确地反映算法的效率。如果我们想准确预估一段代码的运行时间，应该如何操作呢？

1. **确定运行平台**，包括硬件配置、编程语言、系统环境等，这些因素都会影响代码的运行效率。
2. **评估各种计算操作所需的运行时间**，例如加法操作 `+` 需要 1 ns ，乘法操作 `*` 需要 10 ns ，打印操作 `print()` 需要 5 ns 等。
3. **统计代码中所有的计算操作**，并将所有操作的执行时间求和，从而得到运行时间。

例如在以下代码中，输入数据大小为 𝑛 ：

```c
// 在某运行平台下
void algorithm(int n) {
    int a = 2;  // 1 ns
    a = a + 1;  // 1 ns
    a = a * 2;  // 10 ns
    // 循环 n 次
    for (int i = 0; i < n; i++) {   // 1 ns ，每轮都要执行 i++
        printf("%d", 0);            // 5 ns
    }
}
```


根据以上方法，可以得到算法的运行时间为 $(6𝑛+12)$ ns ：

$$1+1+10+(1+5)×𝑛=6𝑛+12$$

但实际上，**统计算法的运行时间既不合理也不现实**。首先，我们不希望将预估时间和运行平台绑定，因为算法需要在各种不同的平台上运行。其次，我们很难获知每种操作的运行时间，这给预估过程带来了极大的难度。

## 统计时间增长趋势

时间复杂度分析统计的不是算法运行时间，**而是算法运行时间随着数据量变大时的增长趋势**。

“时间增长趋势”这个概念比较抽象，我们通过一个例子来加以理解。假设输入数据大小为 𝑛 ，给定三个算法 `A`、`B` 和 `C` ：

```c
// 算法 A 的时间复杂度：常数阶
void algorithm_A(int n) {
    printf("%d", 0);
}
// 算法 B 的时间复杂度：线性阶
void algorithm_B(int n) {
    for (int i = 0; i < n; i++) {
        printf("%d", 0);
    }
}
// 算法 C 的时间复杂度：常数阶
void algorithm_C(int n) {
    for (int i = 0; i < 1000000; i++) {
        printf("%d", 0);
    }
}
```

图 2-7 展示了以上三个算法函数的时间复杂度。

- 算法 `A` 只有 1 个打印操作，算法运行时间不随着 𝑛 增大而增长。我们称此算法的时间复杂度为“常数阶”。
- 算法 `B` 中的打印操作需要循环 𝑛 次，算法运行时间随着 𝑛 增大呈线性增长。此算法的时间复杂度被称为“线性阶”。
- 算法 `C` 中的打印操作需要循环 1000000 次，虽然运行时间很长，但它与输入数据大小 𝑛 无关。因此 `C` 的时间复杂度和 `A` 相同，仍为“常数阶”。

![[复杂度级别关系.png| center | 500]]

相较于直接统计算法的运行时间，时间复杂度分析有哪些特点呢？

- **时间复杂度能够有效评估算法效率**。例如，算法 `B` 的运行时间呈线性增长，在 𝑛>1 时比算法 `A` 更慢，在 $𝑛>1000000$ 时比算法 `C` 更慢。事实上，只要输入数据大小 𝑛 足够大，复杂度为“常数阶”的算法一定优于“线性阶”的算法，这正是<mark class="hltr-blue">时间增长趋势</mark>的含义。
- **时间复杂度的推算方法更简便**。显然，运行平台和计算操作类型都与算法运行时间的增长趋势无关。因此在时间复杂度分析中，我们可以简单地将所有计算操作的执行时间视为相同的“单位时间”，从而将“计算操作运行时间统计”简化为“计算操作数量统计”，这样一来估算难度就大大降低了。
- **时间复杂度也存在一定的局限性**。例如，尽管算法 `A` 和 `C` 的时间复杂度相同，但实际运行时间差别很大。同样，尽管算法 `B` 的时间复杂度比 `C` 高，但在输入数据大小 𝑛 较小时，算法 `B` 明显优于算法 `C` 。在这些情况下，我们很难仅凭时间复杂度判断算法效率的高低。当然，尽管存在上述问题，复杂度分析仍然是评判算法效率最有效且常用的方法。

## 函数渐近上界

给定一个输入大小为 𝑛 的函数：

```c
void algorithm(int n) {
    int a = 1;  // +1
    a = a + 1;  // +1
    a = a * 2;  // +1
    // 循环 n 次
    for (int i = 0; i < n; i++) {   // +1（每轮都执行 i ++）
        printf("%d", 0);            // +1
    }
}
```

设算法的操作数量是一个关于输入数据大小 𝑛 的函数，记为 𝑇(𝑛) ，则以上函数的操作数量为：

$$𝑇(𝑛)=3+2𝑛$$

𝑇(𝑛) 是一次函数，说明其运行时间的增长趋势是线性的，因此它的时间复杂度是线性阶。

我们将线性阶的时间复杂度记为 $𝑂(𝑛)$ ，这个数学符号称为大 𝑂 记号（big-𝑂 notation），表示函数 $𝑇(𝑛)$ 的渐近上界（asymptotic upper bound）。

时间复杂度分析本质上是计算“<mark class="hltr-cyan">操作数量</mark> $𝑇(𝑛)$”的渐近上界，它具有明确的数学定义。

> [!info]
> 若存在正实数 𝑐 和实数 𝑛0 ，使得对于所有的 𝑛>𝑛0 ，均有 𝑇(𝑛)≤𝑐⋅𝑓(𝑛) ，则可认为 𝑓(𝑛) 给出了 𝑇(𝑛) 的一个渐近上界，记为 𝑇(𝑛)=𝑂(𝑓(𝑛)) 。

如图 2-8 所示，计算渐近上界就是寻找一个函数 $𝑓(𝑛)$ ，使得当 𝑛 趋向于无穷大时，$𝑇(𝑛)$ 和 $𝑓(𝑛)$ 处于相同的增长级别，仅相差一个常数项 $𝑐$ 的倍数。

![[计算渐进上界.png| center | 500]]

